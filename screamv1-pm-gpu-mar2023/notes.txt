The pm-gpu runs in se58-mar9 are from a scream repo of March9th, but do have some minor changes in the module versions on the machine (that are going into E3SM).

It effectively has these changes:
https://github.com/E3SM-Project/E3SM/pull/5533

/global/cfs/cdirs/e3sm/ndk/repos/se59-mar13-pm-module-updateMar2023
The git hash: 4934926240

Also summarizing these runs here:
https://acme-climate.atlassian.net/wiki/spaces/EPG/pages/3552935978/Performance+of+SCREAM+ne1024+cases+on+pm-gpu

On pm-gpu, it makes the most sense to use 4 MPI's per node, corresponding to 4 A100's per node.

The pm-gpu cases are using CUDA-aware MPI: 
setenv MPICH_GPU_SUPPORT_ENABLED 1

The two cases noted here:

/pscratch/sd/n/ndk/e3sm_scratch/pm-gpu/se58-mar9/f1024.F2010-SCREAMv1.ne1024pg2_ne1024pg2.se58-mar9.gnugpu.1d.n0512a4xX.nor.ctk117.mpich8124

/pscratch/sd/n/ndk/e3sm_scratch/pm-gpu/se58-mar9/f1024.F2010-SCREAMv1.ne1024pg2_ne1024pg2.se58-mar9.gnugpu.1d.n1024a4xX.nor.ctk117.mpich8124


This is a 512-node case that does not have the minor module version update to show perf is about the same
/pscratch/sd/n/ndk/e3sm_scratch/pm-gpu/se58-mar9/f1024.F2010-SCREAMv1.ne1024pg2_ne1024pg2.se58-mar9.gnugpu.1d.n0512a4xX.nor


These cases were run March 9,10,11 -- and I think shortly after this, NERSC made a change that drastically affects
performance, but was meant to ward off frequent NODE_FAILS.  I have a 384 node case as well, but it hit NODE_FAIL
at the end of the run and does not have detailed timing data. More recent 384-node runs did complete without
NODE_FAIL, but were much slower -- just helping NERSC debug the issue.  NERSC hopes to chaneg the env variable
FI_MR_CUDA_CACHE_MONITOR_ENABLED=0 back to normal (1) after next machine maintenance which should be soon. 
